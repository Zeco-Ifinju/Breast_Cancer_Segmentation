{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the Libraries \n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "# Define the data directory\n",
    "data_dir = '/home/ubuntu/BTS/Breast_Cancer_Segmentation/Dataset_BUSI_with_GT' # change the path of the directory\n",
    "categories = ['benign', 'malignant']\n",
    "\n",
    "# Function to load and visualize data\n",
    "def load_and_visualize_data(data_dir, categories):\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
    "    \n",
    "    for i, category in enumerate(categories):\n",
    "        category_dir = os.path.join(data_dir, category)\n",
    "        images = [f for f in os.listdir(category_dir) if f.endswith('.png') and 'MASK' not in f]\n",
    "        \n",
    "        img_path = os.path.join(category_dir, images[0])\n",
    "        mask_path = os.path.join(category_dir, images[0].replace('.png', '_MASK.png'))\n",
    "        \n",
    "        image = img_to_array(load_img(img_path, color_mode='grayscale'))\n",
    "        mask = img_to_array(load_img(mask_path, color_mode='grayscale'))\n",
    "        \n",
    "        axes[i, 0].imshow(image.squeeze(), cmap='gray')\n",
    "        axes[i, 0].set_title(f'{category.capitalize()} Image')\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        axes[i, 1].imshow(mask.squeeze(), cmap='gray')\n",
    "        axes[i, 1].set_title(f'{category.capitalize()} Mask')\n",
    "        axes[i, 1].axis('off')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Visualize the data\n",
    "load_and_visualize_data(data_dir, categories)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the Data into Train, Validation, and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Function to get image and mask pairs\n",
    "def get_image_mask_pairs(data_dir, categories):\n",
    "    image_paths = []\n",
    "    mask_paths = []\n",
    "    \n",
    "    for category in categories:\n",
    "        category_dir = os.path.join(data_dir, category)\n",
    "        images = [f for f in os.listdir(category_dir) if f.endswith('.png') and 'MASK' not in f]\n",
    "        \n",
    "        for image in images:\n",
    "            img_path = os.path.join(category_dir, image)\n",
    "            mask_path = os.path.join(category_dir, image.replace('.png', '_MASK.png'))\n",
    "            image_paths.append(img_path)\n",
    "            mask_paths.append(mask_path)\n",
    "    \n",
    "    return np.array(image_paths), np.array(mask_paths)\n",
    "\n",
    "# Get image and mask pairs\n",
    "image_paths, mask_paths = get_image_mask_pairs(data_dir, categories)\n",
    "\n",
    "# Split the data\n",
    "train_img_paths, test_img_paths, train_mask_paths, test_mask_paths = train_test_split(image_paths, mask_paths, test_size=0.2, random_state=42)\n",
    "train_img_paths, val_img_paths, train_mask_paths, val_mask_paths = train_test_split(train_img_paths, train_mask_paths, test_size=0.25, random_state=42)  # 0.25 x 0.8 = 0.2\n",
    "\n",
    "# Summary of the splits\n",
    "print(f\"Training set size: {len(train_img_paths)}\")\n",
    "print(f\"Validation set size: {len(val_img_paths)}\")\n",
    "print(f\"Test set size: {len(test_img_paths)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Augment the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Data augmentation\n",
    "data_gen_args = dict(rotation_range=10,\n",
    "                     width_shift_range=0.1,\n",
    "                     height_shift_range=0.1,\n",
    "                     shear_range=0.1,\n",
    "                     zoom_range=0.1,\n",
    "                     horizontal_flip=True,\n",
    "                     fill_mode='nearest')\n",
    "\n",
    "image_datagen = ImageDataGenerator(**data_gen_args)\n",
    "mask_datagen = ImageDataGenerator(**data_gen_args)\n",
    "\n",
    "# Example of applying augmentation to an image and its mask\n",
    "def augment_data(img_path, mask_path, image_datagen, mask_datagen):\n",
    "    img = img_to_array(load_img(img_path, color_mode='grayscale'))\n",
    "    mask = img_to_array(load_img(mask_path, color_mode='grayscale'))\n",
    "    \n",
    "    img = img.reshape((1,) + img.shape)\n",
    "    mask = mask.reshape((1,) + mask.shape)\n",
    "    \n",
    "    img_gen = image_datagen.flow(img, batch_size=1)\n",
    "    mask_gen = mask_datagen.flow(mask, batch_size=1)\n",
    "    \n",
    "    img_aug, mask_aug = next(img_gen), next(mask_gen)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "    axes[0].imshow(img_aug[0].squeeze(), cmap='gray')\n",
    "    axes[0].set_title('Augmented Image')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(mask_aug[0].squeeze(), cmap='gray')\n",
    "    axes[1].set_title('Augmented Mask')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "augment_data(train_img_paths[0], train_mask_paths[0], image_datagen, mask_datagen)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design a U-Net Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def unet_model(input_size=(256, 256, 1)):\n",
    "    inputs = Input(input_size)\n",
    "    \n",
    "    # Contracting Path\n",
    "    c1 = Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    c1 = Conv2D(64, (3, 3), activation='relu', padding='same')(c1)\n",
    "    p1 = MaxPooling2D((2, 2))(c1)\n",
    "    \n",
    "    c2 = Conv2D(128, (3, 3), activation='relu', padding='same')(p1)\n",
    "    c2 = Conv2D(128, (3, 3), activation='relu', padding='same')(c2)\n",
    "    p2 = MaxPooling2D((2, 2))(c2)\n",
    "    \n",
    "    c3 = Conv2D(256, (3, 3), activation='relu', padding='same')(p2)\n",
    "    c3 = Conv2D(256, (3, 3), activation='relu', padding='same')(c3)\n",
    "    p3 = MaxPooling2D((2, 2))(c3)\n",
    "    \n",
    "    c4 = Conv2D(512, (3, 3), activation='relu', padding='same')(p3)\n",
    "    c4 = Conv2D(512, (3, 3), activation='relu', padding='same')(c4)\n",
    "    p4 = MaxPooling2D((2, 2))(c4)\n",
    "    \n",
    "    # Bottleneck\n",
    "    c5 = Conv2D(1024, (3, 3), activation='relu', padding='same')(p4)\n",
    "    c5 = Conv2D(1024, (3, 3), activation='relu', padding='same')(c5)\n",
    "    \n",
    "    # Expanding Path\n",
    "    u6 = UpSampling2D((2, 2))(c5)\n",
    "    u6 = concatenate([u6, c4])\n",
    "    c6 = Conv2D(512, (3, 3), activation='relu', padding='same')(u6)\n",
    "    c6 = Conv2D(512, (3, 3), activation='relu', padding='same')(c6)\n",
    "    \n",
    "    u7 = UpSampling2D((2, 2))(c6)\n",
    "    u7 = concatenate([u7, c3])\n",
    "    c7 = Conv2D(256, (3, 3), activation='relu', padding='same')(u7)\n",
    "    c7 = Conv2D(256, (3, 3), activation='relu', padding='same')(c7)\n",
    "    \n",
    "    u8 = UpSampling2D((2, 2))(c7)\n",
    "    u8 = concatenate([u8, c2])\n",
    "    c8 = Conv2D(128, (3, 3), activation='relu', padding='same')(u8)\n",
    "    c8 = Conv2D(128, (3, 3), activation='relu', padding='same')(c8)\n",
    "    \n",
    "    u9 = UpSampling2D((2, 2))(c8)\n",
    "    u9 = concatenate([u9, c1])\n",
    "    c9 = Conv2D(64, (3, 3), activation='relu', padding='same')(u9)\n",
    "    c9 = Conv2D(64, (3, 3), activation='relu', padding='same')(c9)\n",
    "    \n",
    "    outputs = Conv2D(1, (1, 1), activation='sigmoid')(c9)\n",
    "    \n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Initialize the model\n",
    "model = unet_model()\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "def create_generators(image_paths, mask_paths, batch_size, image_datagen, mask_datagen):\n",
    "    while True:\n",
    "        for i in range(0, len(image_paths), batch_size):\n",
    "            batch_img_paths = image_paths[i:i + batch_size]\n",
    "            batch_mask_paths = mask_paths[i:i + batch_size]\n",
    "            \n",
    "            img_batch = np.array([img_to_array(load_img(img_path, color_mode='grayscale', target_size=(256, 256))) for img_path in batch_img_paths])\n",
    "            mask_batch = np.array([img_to_array(load_img(mask_path, color_mode='grayscale', target_size=(256, 256))) for mask_path in batch_mask_paths])\n",
    "            \n",
    "            img_batch = img_batch.astype('float32') / 255.0\n",
    "            mask_batch = mask_batch.astype('float32') / 255.0\n",
    "            \n",
    "            img_gen = image_datagen.flow(img_batch, batch_size=batch_size, shuffle=True, seed=42)\n",
    "            mask_gen = mask_datagen.flow(mask_batch, batch_size=batch_size, shuffle=True, seed=42)\n",
    "            \n",
    "            yield next(img_gen), next(mask_gen)\n",
    "\n",
    "# Generators for training and validation\n",
    "train_generator = create_generators(train_img_paths, train_mask_paths, 16, image_datagen, mask_datagen)\n",
    "val_generator = create_generators(val_img_paths, val_mask_paths, 16, image_datagen, mask_datagen)\n",
    "\n",
    "# Callbacks\n",
    "checkpoint = ModelCheckpoint('unet_model.h5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True) # stop training when the model stops learning\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(train_img_paths) // 16,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=len(val_img_paths) // 16,\n",
    "    epochs=50,\n",
    "    callbacks=[checkpoint, early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    # Plot training & validation loss values\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "    \n",
    "    # Plot training & validation accuracy values\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Visualize the performance\n",
    "plot_training_history(history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Model Performance on the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "def evaluate_model_on_test_set(model, test_img_paths, test_mask_paths):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    for img_path, mask_path in zip(test_img_paths, test_mask_paths):\n",
    "        img = img_to_array(load_img(img_path, color_mode='grayscale', target_size=(256, 256)))\n",
    "        mask = img_to_array(load_img(mask_path, color_mode='grayscale', target_size=(256, 256)))\n",
    "        \n",
    "        img = img.reshape((1,) + img.shape)\n",
    "        pred_mask = model.predict(img)\n",
    "        pred_mask = (pred_mask > 0.5).astype(int)\n",
    "        \n",
    "        y_true.append(mask.flatten())\n",
    "        y_pred.append(pred_mask.flatten())\n",
    "    \n",
    "    y_true = np.concatenate(y_true)\n",
    "    y_pred = np.concatenate(y_pred)\n",
    "    \n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    print(f'Accuracy on the test set: {accuracy * 100:.2f}%')\n",
    "    print(f'Confusion Matrix:\\n{conf_matrix}')\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "evaluate_model_on_test_set(model, test_img_paths, test_mask_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_predictions(model, test_img_paths, test_mask_paths, num_samples=5):\n",
    "    \"\"\"\n",
    "    Visualizes the test images, ground truth masks, and predicted masks.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: Trained U-Net model.\n",
    "    - test_img_paths: List of paths to test images.\n",
    "    - test_mask_paths: List of paths to ground truth masks for the test images.\n",
    "    - num_samples: Number of samples to visualize.\n",
    "    \"\"\"\n",
    "    for i in range(num_samples):\n",
    "        # Load and preprocess the test image and its corresponding mask\n",
    "        img_path = test_img_paths[i]\n",
    "        mask_path = test_mask_paths[i]\n",
    "        \n",
    "        img = img_to_array(load_img(img_path, color_mode='grayscale', target_size=(256, 256))) / 255.0\n",
    "        mask = img_to_array(load_img(mask_path, color_mode='grayscale', target_size=(256, 256))) / 255.0\n",
    "        \n",
    "        # Predict the mask\n",
    "        pred_mask = model.predict(img.reshape(1, 256, 256, 1))\n",
    "        pred_mask = (pred_mask > 0.5).astype(int).reshape(256, 256)\n",
    "        \n",
    "        # Plot the image, ground truth mask, and predicted mask\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        \n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.imshow(img.squeeze(), cmap='gray')\n",
    "        plt.title('Test Image')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.imshow(mask.squeeze(), cmap='gray')\n",
    "        plt.title('Ground Truth Mask')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.imshow(pred_mask, cmap='gray')\n",
    "        plt.title('Predicted Mask')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "# Example usage:\n",
    "visualize_predictions(model, test_img_paths, test_mask_paths, num_samples=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "def visualize_predictions_with_circles(model, test_img_paths, test_mask_paths, num_samples=5):\n",
    "    \"\"\"\n",
    "    Visualizes the test images, ground truth masks, and predicted masks with circles around the tumor region.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: Trained U-Net model.\n",
    "    - test_img_paths: List of paths to test images.\n",
    "    - test_mask_paths: List of paths to ground truth masks for the test images.\n",
    "    - num_samples: Number of samples to visualize.\n",
    "    \"\"\"\n",
    "    for i in range(num_samples):\n",
    "        # Load and preprocess the test image and its corresponding mask\n",
    "        img_path = test_img_paths[i]\n",
    "        mask_path = test_mask_paths[i]\n",
    "        \n",
    "        img = img_to_array(load_img(img_path, color_mode='grayscale', target_size=(256, 256))) / 255.0\n",
    "        mask = img_to_array(load_img(mask_path, color_mode='grayscale', target_size=(256, 256))) / 255.0\n",
    "        \n",
    "        # Predict the mask\n",
    "        pred_mask = model.predict(img.reshape(1, 256, 256, 1))\n",
    "        pred_mask = (pred_mask > 0.5).astype(int).reshape(256, 256)\n",
    "        \n",
    "        # Convert image, mask, and prediction to uint8 for contour detection\n",
    "        img_uint8 = (img.squeeze() * 255).astype(np.uint8)\n",
    "        mask_uint8 = (mask.squeeze() * 255).astype(np.uint8)\n",
    "        pred_mask_uint8 = (pred_mask * 255).astype(np.uint8)\n",
    "        \n",
    "        # Find contours for ground truth mask and predicted mask\n",
    "        contours_gt, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        contours_pred, _ = cv2.findContours(pred_mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        \n",
    "        # Draw circles around the largest contour (assumed to be the tumor) for ground truth and prediction\n",
    "        def draw_circle(image, contours, color):\n",
    "            if contours:\n",
    "                # Find the largest contour\n",
    "                c = max(contours, key=cv2.contourArea)\n",
    "                # Get the minimum enclosing circle around the contour\n",
    "                (x, y), radius = cv2.minEnclosingCircle(c)\n",
    "                center = (int(x), int(y))\n",
    "                radius = int(radius)\n",
    "                # Draw the circle\n",
    "                cv2.circle(image, center, radius, color, 2)\n",
    "        \n",
    "        # Draw circles on copies of the images for visualization\n",
    "        img_with_circle = cv2.cvtColor(img_uint8, cv2.COLOR_GRAY2RGB)\n",
    "        mask_with_circle = cv2.cvtColor(mask_uint8, cv2.COLOR_GRAY2RGB)\n",
    "        pred_mask_with_circle = cv2.cvtColor(pred_mask_uint8, cv2.COLOR_GRAY2RGB)\n",
    "        \n",
    "        # Green for ground truth, blue for prediction\n",
    "        draw_circle(img_with_circle, contours_gt, (0, 255, 0))\n",
    "        draw_circle(mask_with_circle, contours_gt, (0, 255, 0))\n",
    "        draw_circle(pred_mask_with_circle, contours_pred, (255, 0, 0))\n",
    "        \n",
    "        # Plot the image, ground truth mask, and predicted mask\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        \n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.imshow(img_with_circle)\n",
    "        plt.title('Test Image with Tumor Circle')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.imshow(mask_with_circle)\n",
    "        plt.title('Ground Truth Mask with Tumor Circle')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.imshow(pred_mask_with_circle)\n",
    "        plt.title('Predicted Mask with Tumor Circle')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "# Example usage:\n",
    "visualize_predictions_with_circles(model, test_img_paths, test_mask_paths, num_samples=5)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
